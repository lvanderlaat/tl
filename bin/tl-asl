#!/usr/bin/env python


"""
Amplitude source location
"""


# Python Standard Library
import os
import multiprocessing

from itertools import repeat
from linecache import getline

# Other dependencies
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tl

from numba import jit

# Local files


__author__ = 'Leonardo van der Laat'
__email__  = 'laat@umich.edu'


@jit(nopython=True)
def locate(Q, alpha, beta, f, a, x, y, z, xxx, yyy, zzz):
    B = np.pi*f/Q/beta

    res = np.zeros(xxx.shape)

    n_stations = len(a)

    for i in range(n_stations-1):
        d_i = np.sqrt((xxx-x[i])**2 + (yyy-y[i])**2 + (zzz-z[i])**2)
        for j in range(i+1, n_stations):
            d_j = np.sqrt((xxx-x[j])**2 + (yyy-y[j])**2 + (zzz-z[j])**2)

            r_obs = a[i]/a[j]

            r_calc = (np.exp(-B*d_i)/d_i**alpha) / (np.exp(-B*d_j)/d_j**alpha)

            res += (r_calc - r_obs)**2
    return np.sqrt(res)


def work(
    Q, alpha, beta, f, a, x, y, z, xmin, xmax, ymin, ymax, zmin, zmax, cellsize
):
    # Create grid
    xxx, yyy, zzz = np.meshgrid(
        np.arange(xmin, xmax+cellsize, cellsize),
        np.arange(ymin, ymax+cellsize, cellsize),
        np.arange(zmin, zmax+cellsize, cellsize)
    )

    res = locate(Q, alpha, beta, f, a, x, y, z, xxx, yyy, zzz)
    idx = np.unravel_index(res.argmin(), res.shape)
    loc = np.array([xxx[idx], yyy[idx], zzz[idx]])
    print(loc)
    return loc, res[idx]


def main():
    # Parse arguments
    args = tl.utils.parse_args()

    # Parse configuration YAML file
    c = tl.config.read(args.configfile)


    # Output folder directory
    folderpath = tl.utils.create_folder(c.io.output_dir, f'ASL', c.io.overwrite)

    # Channels
    channels = pd.read_csv(c.amplitude.channels_csv, comment='/')
    channels = channels[channels.channel.str[-1] == 'Z']
    channels.reset_index(inplace=True)

    # features metadata
    meta = tl.features.to_dataframe(channels, c.amplitude.bands)

    # Read metadata
    meta = pd.read_csv(metadata_file)
    meta = meta[(meta.freqmin == band[0]) & (meta.freqmax == band[1])]

    # if auto_limits:
    #     xmin = channels.x.min() - 2*cellsize
    #     xmax = channels.x.max() + 2*cellsize
    #     ymin = channels.y.min() - 2*cellsize
    #     ymax = channels.y.max() + 2*cellsize

    # Correct site effect
    for i, row in meta.iterrows():
        df[row.key] /= row.cf

    args_parallel = zip(
        repeat(Q), repeat(alpha), repeat(beta), repeat(f),
        df[meta.key].values,
        repeat(meta.x.values), repeat(meta.y.values), repeat(meta.z.values),
        repeat(xmin), repeat(xmax),
        repeat(ymin), repeat(ymax),
        repeat(zmin), repeat(zmax),
        repeat(cellsize)
    )
    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:
        results = pool.starmap(work, args_parallel)

    locs, res = list(map(list, zip(*results)))
    locs = np.array(locs)

    out = pd.DataFrame(locs, columns='x y z'.split())
    out['res'] = res
    out['eventid'] = eventids

    out.to_csv(os.path.join(folderpath, 'test_pred.csv'), index=False)
    return



if __name__ == '__main__':
    zmin = -1200
    zmax = 800
    cellsize = 100
    band = [0.38, 1.2]
    Q, alpha, beta, f = 11, 0.5, 1873, 0.7

    metadata_file = '/Users/laat/Dropbox (University of Michigan)/paper/dat/SITE/meta.csv'

    filename = '/Users/laat/Dropbox (University of Michigan)/paper/dat/NET_ALL/PHASE_1/MODEL/test_pred.csv'
    eventids = pd.read_csv(filename).eventid

    # Read amplitude data
    df = pd.read_csv('/Users/laat/Dropbox (University of Michigan)/paper/dat/AMP_EQ/data.csv')
    df = df[df.eventid.isin(eventids)]

    xmin = df.x.min() - 2*cellsize
    xmax = df.x.max() + 2*cellsize
    ymin = df.y.min() - 2*cellsize
    ymax = df.y.max() + 2*cellsize

    main()
