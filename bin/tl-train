#!/usr/bin/env python


"""
Train a linear regression mode for earthquake/tremor location
"""


# Python Standard Library
import os
import pickle
import warnings

# Other dependencies
import numpy as np
import pandas as pd

from sklearn.linear_model import RidgeCV, Ridge
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Local files
import tl


__author__ = 'Leonardo van der Laat'
__email__  = 'laat@umich.edu'


def main():
    # Parse arguments
    args = tl.utils.parse_args(datatypes=['SY', 'EQ'])

    # Logger
    logger = tl.utils.get_logger()

    # Get configuration
    c = tl.config.read(args.configfile)

    # Create output folder
    folderpath = tl.utils.create_folder(
        c.io.output_dir, f'MODEL_{args.datatype}', c.io.overwrite
    )

    # Load metadata
    channels = pd.read_csv(c.amplitude.channels_csv, comment='/')

    # Amplitude data and metadata
    meta = tl.features.to_dataframe(channels, c.amplitude.bands)
    if args.datatype == 'SY':
        # Get metadata for amplitude data
        bands = [c.amplitude.bands[0]]
        meta = tl.features.to_dataframe(channels, bands)
        meta = meta[meta.channel.isin(['HHZ', 'EHZ'])]
        meta = meta.drop_duplicates(subset='station')

    df   = pd.read_csv(os.path.join(c.engineer.amp_folder, 'data.csv'))

    # Filter data
    df = tl.catalog.filter(
        df,
        magnitude_min=c.dataset.magnitude_min,
        magnitude_max=c.dataset.magnitude_max,
        n_events=c.dataset.n_events
    )

    # Feature engineering
    logger.info('Feature engineering...')
    metadata, features = tl.features.engineer(
        df, meta,
        c.engineer.ratio_same_station,
        c.engineer.ratio_diff_bands,
        c.engineer.transformations,
        c.performance.max_workers
    )

    logger.info(f'{len(metadata)} features')

    # Get rid of missing values, in the future imputation can be explored
    features.replace([np.inf, -np.inf], np.nan, inplace=True)

    # To NumPy arrays for SciKit-Learn
    X = features[metadata.key].values
    y = features[['x', 'y', 'z']].values

    # Impute NaN values
    logger.info('Imputing NaN values')
    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
    X = imputer.fit_transform(X)

    # Normalize location (Kriegerowski et al., 2019)
    scaler_y = StandardScaler()
    y = scaler_y.fit_transform(y)

    # Test-train
    logger.info(f'Training with {(1-c.train.test_size)*100}% of the dataset')

    eventids = features.eventid.values

    # Split training and testing data sets
    X_train, X_test, y_train, y_test, evid_train, evid_test = train_test_split(
        X, y, eventids, test_size=c.train.test_size,
        random_state=c.train.random_state
    )

    logger.info(
        f'n={X.shape[0]}, n(train)={X_train.shape[0]}, n(test)={X_test.shape[0]}'
    )

    # Model
    model = RidgeCV(alphas=[1e2, 1e3, 1e4])

    # Normalize
    scaler_X = StandardScaler()
    scaler_X.fit(X_train)
    X_train = scaler_X.transform(X_train)
    X_test  = scaler_X.transform(X_test)

    # Fit the model
    logger.info('Training...')
    model.fit(X_train, y_train)

    alpha = model.alpha_
    logger.info(f'Best alpha: {model.alpha_}')

    # Predict test set
    logger.info('Predicting test set...')
    y_pred = model.predict(X_test)

    # Re-scale
    y_pred = scaler_y.inverse_transform(y_pred)
    y_test = scaler_y.inverse_transform(y_test)

    # Error
    misfits, interval, stdev = tl.train.get_scores(y_test, y_pred)
    logger.info(f'Test error: {int(misfits.mean())} m')
    logger.info(f'Standard deviation (xyz): {stdev.astype(int)} m')
    logger.info(f'Prediction interval (xyz): {interval.astype(int)} m')

    # Test Output Data
    df_true, df_pred = tl.train.results_to_df(y_test, y_pred, misfits, evid_test)
    df_true.to_csv(os.path.join(folderpath, 'test_true.csv'), index=False)
    df_pred.to_csv(os.path.join(folderpath, 'test_pred.csv'), index=False)

    # Plots
    fig = tl.plot.misfits_histograms(df_true, df_pred)
    fig.savefig(os.path.join(folderpath, 'test_misfits_histograms.pdf'))

    sta = channels.drop_duplicates(subset='station')
    fig = tl.plot.locs_test_sample(df_true, df_pred, sta)
    fig.savefig(os.path.join(folderpath, 'test_locs_sample.pdf'), dpi=300)

    fig = tl.plot.locs_true_pred(df_true, df_pred, sta, s=5)
    fig.savefig(os.path.join(folderpath, 'test_locs_all.pdf'), dpi=300)

    # Total data set
    logger.info('Training with the whole data set...')
    # Model
    model = Ridge(alpha=alpha)

    # Normalize
    scaler_X = StandardScaler()
    scaler_X.fit(X)
    X = scaler_X.transform(X)

    # Fit the model
    logger.info('Training...')
    model.fit(X, y)

    # Predict test set
    logger.info('Predicting...')
    y_pred = model.predict(X)

    # Re-scale
    y = scaler_y.inverse_transform(y)
    y_pred = scaler_y.inverse_transform(y_pred)

    # Error
    misfits, interval, stdev = tl.train.get_scores(y, y_pred)
    logger.info(f'Total error: {int(misfits.mean())} m')
    logger.info(f'Standard deviation (xyz): {stdev.astype(int)} m')
    logger.info(f'Prediction interval (xyz): {interval.astype(int)} m')

    # Test Output Data
    df_true, df_pred = tl.train.results_to_df(y, y_pred, misfits, eventids)

    # Save the scaler and the model
    for name, obj in zip(
        'scaler_X scaler_y model'.split(), [scaler_X, scaler_y, model]
    ):
        pickle.dump(obj, open(os.path.join(folderpath, f'{name}.pkl'), 'wb'))

    tl.utils.write_conf(c, folderpath)

    if args.datatype == 'SY':
        # Where the EW slice is gonna be
        y_plot = (sta.y.min() + sta.y.max())/2


        # Compute misfit distribution
        dfh, dfv, xh, yh, zv = tl.synth.distribution(
            meta,
            model, scaler_X, scaler_y,
            c.projection.epsg,
            c.synth.lonmin, c.synth.lonmax, c.synth.latmin, c.synth.latmax,
            c.synth.zmin, c.synth.zmax, c.synth.step, c.synth.z_plot, y_plot,
            c.synth.f, c.synth.Q, c.synth.beta, c.synth.alpha,
            c.engineer.transformations,
            c.performance.max_workers
        )

        # Plot misfit distribution
        fig = tl.plot.synth_misfit(
            dfh, dfv, sta, y_plot, c.synth.z_plot, xh, yh, xh, zv,
            c.synth.step
        )
        fig.savefig(os.path.join(folderpath, 'misfit.pdf'), dpi=300)
    return


if __name__ == '__main__':
    warnings.filterwarnings('ignore')
    main()
